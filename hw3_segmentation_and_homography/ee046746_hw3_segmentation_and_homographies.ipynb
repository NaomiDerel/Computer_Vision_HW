{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "vUqTjdlLMmvU"
   },
   "source": [
    "# <img src=\"https://img.icons8.com/bubbles/100/000000/3d-glasses.png\" style=\"height:50px;display:inline\"> EE 046746 - Technion - Computer Vision\n",
    "\n",
    "\n",
    "## Homework 3 - Segmentation and Homographies\n",
    "---\n",
    "\n",
    "### <a style='color:red'> Due Date: 08.01.2025 </a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "NSeuG0qiJecp"
   },
   "source": [
    "### <img src=\"https://img.icons8.com/bubbles/50/000000/upload-to-cloud.png\" style=\"height:50px;display:inline\"> Submission Guidelines\n",
    "---\n",
    "#### READ THIS CAREFULLY\n",
    "* Submission only in **pairs**.\n",
    "* **No handwritten submissions**.\n",
    "* You can choose your working environment:\n",
    "    * You can work in a `Jupyter Notebook`, locally with <a href=\"https://www.anaconda.com/distribution/\">Anaconda</a> or online on <a href=\"https://colab.research.google.com/\">Google Colab</a>\n",
    "  * **Important**: Colab also supports running code on GPU, so if you don't have one, Colab is the way to go. To enable GPU on Colab, in the menu: `Runtime` $\\rightarrow$  `Change Runtime Type` $\\rightarrow$`GPU`.\n",
    "    * You can work in a Python IDE such as <a href=\"https://www.jetbrains.com/pycharm/\">PyCharm</a> or <a href=\"https://code.visualstudio.com/\">Visual Studio Code</a>.\n",
    "        * Both also allow opening/editing Jupyter Notebooks.\n",
    "\n",
    "* Make sure you submit your exercise according to the requirements in the <a href=\"https://moodle.technion.ac.il/pluginfile.php/1823033/mod_resource/content/2/Homework%20guidelines.pdf\">\"Homework submission guidelines\"</a> file that appears in the course website (Moodle). \n",
    "* **The code should run both on CPU and GPU without manual modifications**, require no special preparation and run on every computer.\n",
    "* Be precise, we expect on point answers.\n",
    "* Submission on the course website (Moodle).\n",
    "* Bonuses are up to 10 points total (together). Maximum grade for submission is 105."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "LRwFRb5lGnT6"
   },
   "source": [
    "### <img src=\"https://img.icons8.com/dusk/64/000000/python.png\" style=\"height:50px;display:inline\"> Python Libraries\n",
    "---\n",
    "\n",
    "* `numpy`\n",
    "* `matplotlib`\n",
    "* `pytorch` (and `torchvision`)\n",
    "* `opencv` (or `scikit-image`)\n",
    "* `scikit-learn`\n",
    "* Anything else you need (`PIL`, `os`, `pandas`, `csv`, `json`,...)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Quick note\n",
    "in this task, you will be required to use Deep learning segmentation methods. for that, you can use any method that has been presented in class, including but not limited to SegmentAnything,Mask-RCNN, and more.\n",
    "if you choose SegmentAnything, use the following link to see an example for usage: <a href=\"https://github.com/facebookresearch/segment-anything\">link here</a>. Note: if you use SAM, you must in order to use it, download pre-trained weights. please note in the report which model-type you chose, the link to the download. DO NOT include the pre-traiend weights"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Installing segment-anything package:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "! pip install git+https://github.com/facebookresearch/segment-anything.git "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "NredfyrdGnT6"
   },
   "source": [
    "### <img src=\"https://img.icons8.com/bubbles/50/000000/checklist.png\" style=\"height:50px;display:inline\"> Tasks\n",
    "---\n",
    "* In all tasks, you should document your process and results in a report file (which will be saved as `.pdf`). \n",
    "* You can reference your code in the report file, but no need for actual code in this file, the code is submitted in a seprate folder as explained above."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "7vecKhsUGnT7"
   },
   "source": [
    "#### Part 1 - Classic Vs. Deep Learning-based Semantic Segmentation - bonus\n",
    "---\n",
    "In this part you are going to compare classic methods for segmentation to deep learning-based methods. \n",
    "\n",
    "1. Load the images in the `./data/frogs` and `./data/horses` folders and display them.\n",
    "2. Pick 1 classic method for segmentation and 1 deep learning-based method and segment the given images. Display the results.\n",
    "    * **Briefly** summarize each method you picked and discuss the advantages and disadvantages of each method. In your answer, relate to the results you received in this section.\n",
    "    * You can use a ready implementation from the internet or OpenCV, no need to implement it yourselves.\n",
    "    * Note: the classic method **must not** use any neural network.\n",
    "3. Pick 3 images (download from the internet or take them yourself) that satisfy the following, and dispaly them:\n",
    "    * One image of a living being (human, animal,...).\n",
    "    * One image of commonly-used object (car, chair, smartphone, glasses,...).\n",
    "    * One image of not-so-commonly-used object (fire extinguisher, satellite,... **BE CREATIVE**).\n",
    "4. Apply each method (one classic and one deep learning-based) on the 3 images. Display the results (mask and segmented image).\n",
    "    * Which method performed better on each image? Describe your thoughts on why one method is better than the other.\n",
    "    * For the classic method you can change parameters per-image, document them in the report.\n",
    "    * You can add manual post-processing to get a mask if needed. If you do that, document in your report \"how hard\" you had to work in the post-processing stage, as it's an indication of the quality of the method.\n",
    "5. As you probably have noticed, segmentation can be rough around the edges, i.e., the mask is not perfect and may be noisy around the edges. What can be done to fix or at least alleviate this problem? Your suggestions can be in pre-processing, inside the segmentation algorithm or in post-processing."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "1ktvSG-6GnT9"
   },
   "source": [
    "#### Part 2 - Jurrasic Fishbach - bonus\n",
    "---\n",
    "In this part you are going to apply segmentation on a video, and integrate with other elements.\n",
    "\n",
    "<img src=\"https://lh3.googleusercontent.com/5zF16wl-tdE1FZCuVsrNxHWzfa6uXo4hYE_BGabKnGovw2W-bIT9gnZlAgU8nUoK=w412-h220-rw\">\n",
    "\n",
    "1. Film a short video of yourself (you can use your phone for that), but without too much camera movement. You on the other hand, can move however you want (we expect you to). Convert the video to frames and resize the images for a reasonable not too high resolution (lower than 720p ~ 1280x720 pixles). You can use the function in `./code/frame_video_convert.py` to help you. Display 2 frames in the report.\n",
    "2. Segment yourself out of the video (frame-by-frame) using one of the methods (classic or deep). Display 2 frames in the report. \n",
    "3. Pick one of the objects in the supplied videos file (`./data/video_models`), convert it to images and segement it out using one of the methods from Part 1(classic or deep). Display 2 frames in the report. You can choose another object from: https://pixabay.com/videos/search/green%20screen/.\n",
    "    * Explain how you performed the sementation for this specific type of video (i.e., green-screen videos). Did you  use a simple/classic method? Deep method? Combined both?\n",
    "4. Put it all together - pick a background, put yourself and the segemented object on the background. Stich it frame-by-frame (don't make the video too long or it will take a lot of time, 10secs maximum). Display 2 frames of the result in your report. Convert the frames back to video. You can use the function in `frame_video_convert.py` to help you.\n",
    "    * Tip: To make it look good, you can resize the images, create a mapping from pixel locations in the original image to pixels locations in the new image.\n",
    "    * You should submit the final video in the `./output` folder (**MANDATORY**).\n",
    "    * We expect some creative results, this can benefit you a lot when you want to demonstrate your Computer Vision abilities."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "9nCiW9SDHEZp"
   },
   "source": [
    "\n",
    "#### Part 3 - Planar Homographies - mandatory :\n",
    "---\n",
    "After we saw how descriptors are implemented and performed, now we will see how to use them for homographis.\n",
    "\n",
    "In this part you will implement an image stitching algorithm, and will learn how to stitch several images of the same scene into a panorama. First, we’ll concentrate on the case of two images and then extend to several images.\n",
    "\n",
    "For the following tasks:\n",
    "- **You are not allowed to use OpenCV/Scipy or any other \"ready to use\" functions when you are asked to implement a function (you can still use the functions to save and load images).**\n",
    "- For each step add illustration images to your report.\n",
    "- You can demonstrate your steps using `incline_L.jpg` and `incline_R.jpg` images, or any other relevant example images (unless specified otherwise)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "FUGJfL58HQg0"
   },
   "source": [
    "#### Planar Homographies: Theory review\n",
    "---\n",
    "Suppose we have two cameras $C_1$ and $C_2$ looking at a common plane $Π$ in 3D space. Any 3D point $P$ on $Π$ generates a projected 2D point located at $p ≡ (x,y,1)^T$ on the ﬁrst camera $C_1$ and $q ≡ (u,v,1)^T$ on the second camera $C_2$. Since $P$ is conﬁned to the plane $Π$, we expect that there is a relationship between $p$ and $q$. In particular, there exists a common $3 × 3$ matrix $H$, so that for any $P$, the following conditions holds: \n",
    "\\begin{align}\n",
    "\\text{(1) }q ≡ Hp\n",
    "\\end{align}\n",
    "\n",
    "\n",
    "\n",
    "We call this relationship *'planar homography'*. Recall that both $p$ and $q$ are in homogeneous coordinates and the equality $≡$ means $p$ is proportional to $Hq$ (recall homogeneous coordinates). It turns out this relationship is also true for cameras that are related by pure rotation without the planar constraint. \n",
    "\n",
    "##### **Matched points:**\n",
    "---\n",
    "\n",
    "Given a set of points $p = \\{p_1,p_2,...,p_N\\}$ in an image taken by camera $C_1$ and corresponding points $q = \\{q_1,q_2,...,q_N\\}$ in an image taken by $C_2$. Suppose we know there exists an unknown homography $H$ between corresponding points for all $i ∈\\{1,2,...,N\\}$. This formally means that $\\exists H$ such that: \n",
    "\n",
    " \n",
    "\\begin{equation*} \n",
    "\\text{(2) } q^i ≡ Hp^i\n",
    "\\end{equation*}\n",
    "\n",
    "\n",
    "where $p^i = (x_i,y_i,1)$ and $q^i = (u_i,v_i,1)$ are homogeneous coordinates of image points each from an image taken with $C_1$ and $C_2$ respectively.\n",
    "\n",
    "* Given $N$ correspondences in $p$ and $q$ and using Equation 2, we derived a set of $2N$ independent linear equations in the form:\n",
    "\n",
    "\n",
    "\\begin{equation*} \n",
    "\\text{(3) } Ah = 0\n",
    "\\end{equation*}\n",
    "\n",
    "\n",
    "where $h$ is a vector of the elements of $H$ and $A$ is a matrix composed of elements derived from the point coordinates:  \n",
    "\n",
    "$$ \\begin{bmatrix} &&&&&\\dots\\\\ x_i & y_i & 1 & 0 & 0 & 0 & -x_iu_i&-y_iu_i& -u_i \\\\ 0&0&0&x_i&y_i&1 & -x_iv_i&-y_iv_i& -v_i \\\\ &&&&&\\dots \\end{bmatrix} \n",
    "\\begin{bmatrix} h_1\\\\h_2\\\\h_3\\\\ h_4 \\\\ h_5 \\\\ h_6 \\\\ h_7\\\\h_8\\\\h_9\\\\ \\end{bmatrix} = \\begin{bmatrix} \\dots \\\\ 0 \\\\ 0  \\\\ \\dots \\end{bmatrix}$$\n",
    "\n",
    "\n",
    "Each point pair contributes 2 equations and therefore we need at least 4 matches."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "APUL270kHbKl"
   },
   "source": [
    "#### Planar Homographies: Practice \n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "KLiRoyCH-x7a"
   },
   "source": [
    "#### Quick detour- Feature Descriptor\n",
    "---\n",
    "In this part we are going to use **Scale-Invariant Feature Transform** (SIFT). We will use OpenCV <a href=\"https://docs.opencv.org/3.4/db/d27/tutorial_py_table_of_contents_feature2d.html\">[2]</a> for the implementation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "sSuye_q_aXZz"
   },
   "source": [
    "##### 3.0.1 SIFT Implementaion\n",
    "---\n",
    "Implement the following function:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "283hBhqCiZlP"
   },
   "outputs": [],
   "source": [
    "def SIFT_descriptor(img):\n",
    " # Returns the SIFT descriptor keypoints of an image,and draw its detected keypoints\n",
    " # INPUTS\n",
    " #      img                 - An image read by cv2.imread()\n",
    " # OUTPUTS\n",
    " #      sift_descriptor     - The SIFT descrptor computed by OpenCV\n",
    " #      sift_keypoints      - The SIFT keypoints computed by OpenCV\n",
    "  \"\"\"\n",
    "  Your code here\n",
    "  \"\"\"\n",
    "  return sift_keypoints, sift_descriptor"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "8wtYmwexiZ0C"
   },
   "source": [
    "The function get an image ,returns its SIFT descriptor and keypoints, and draw the detected keypoints over the image.\n",
    "\n",
    "Plot the results obtained for the `model_chickenbroth` image and another `chickenbroth` image (Use the same image from previous section).\n",
    "\n",
    "\n",
    "* Implementaton guidance:\n",
    "    * Use `sift = cv2.xfeatures2d.SIFT_create()` to instantiate the SIFT detector. \n",
    "    * Detect and compute SIFT keypoints and descriptors by `sift_keypoint, sift_descriptor = sift.detectAndCompute(img,None)`.\n",
    "    * Draw the keypoints over the image by using `cv2.drawKeypoints()`.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "CRREd5kCoGxG"
   },
   "source": [
    "##### 3.1 - Finding corresponding points using SIFT: \n",
    "---\n",
    "Use the guidelines from 3.0.1 and implement the function `getPoints_SIFT()`, which gets two images and outputs `p1,p2` SIFT keypoints, where `p1[j],p2[j]` are pairs of cooresponding points between `im1` and `im2`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "lBzHTC1Qongt"
   },
   "outputs": [],
   "source": [
    "def getPoints_SIFT(im1, im2):\n",
    "    \"\"\"\n",
    "    Your code here\n",
    "    \"\"\"\n",
    "    return p1,p2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "SgxyWbyX3DjV"
   },
   "source": [
    "Inputs: `im1` and `im2` are two 2D grayscale images. \n",
    "\n",
    "Output: `p1` and `p2` should be $2\\times N$ matrices of corresponding $(x,y)^T$ coordinates between two images ($N$ is the number of corrosponding points you want to extract.). \n",
    "\n",
    "* You can also use color images instead of grayscale images, just state it in the report."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "9P8oVI1mjM8y"
   },
   "source": [
    "#### 3.2 - Calculate transformation: \n",
    "---\n",
    "\n",
    "Implement a function that gets a set of matching points between two images and calculates the transformation between them. The transformation should be $3\\times3$ $H$ homogenous matrix such that for each point in image $p\\in C_1$, there would be a transformation in image $C_2$ such that $p=Hq$, $q\\in C_2$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "JC69sOtijOh6",
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "def computeH(p1, p2):\n",
    "    \"\"\"\n",
    "    Your code here\n",
    "    \"\"\"\n",
    "    return H2to1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "YLy8KwMzjRyM"
   },
   "source": [
    "Inputs: `p1` and `p2` should be $2\\times N$ matrices of corresponding $(x,y)^T$ coordinates between two images. \n",
    "\n",
    "Outputs: `H2to1` should be a $3\\times 3$ matrix encoding the homography that best matches the linear equation derived above for Equation 2. \n",
    "\n",
    "Hint: Remember that a homography is only determined up to scale. The `numpy`'s functions `eig()` or `svd()` will be useful. Note that this function can be written without an explicit for-loop over the data points.\n",
    "\n",
    "*Hint for debugging*: A good test of your code is to check that the homography of an image with itself is an identity.\n",
    "\n",
    "* Implement the computation function, describe and explain your implementation.\n",
    "* Show that the transformation is correct by selecting arbitrary points in the first image and projecting them to the second image."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3.3 - Image warping:\n",
    "---\n",
    "Implement a function that gets an input image and a transformation matrix H and returns the warped image. Please note that after the warping, there will be coordinates that won’t be integers (e.g. sub-pixels). Therefore you will need to interpolate between neighboring pixels. For color images, warp the image for each color channel and then connect them together. In order to avoid holes, use inverse warping.\n",
    "\n",
    "Implement the wrapping function using numpy and SciPy interp2d() or RegularGridInterpolator() function .\n",
    "Discuss the influences of different interpolations kinds {‘linear’, ‘cubic’}.\n",
    "Note: When performing a multi-step algorithm, you need to demonstrate and explain each of those additional improvments.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "def warpH(im1, H, out_size):\n",
    "    \"\"\"\n",
    "    Your code here\n",
    "    \"\"\"\n",
    "    return warp_im1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Inputs: `im1` is a colored image. `H` is a  matrix encoding the homography between im1 and im2. `out_size` is the size of the wanted output (new_imH,new_imW).\n",
    "\n",
    "Output: `warp_im1` is the warped image im1 including empty background (zeros)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "yhwjDU9Sjajn"
   },
   "source": [
    "##### 3.4 - Panorama stitching: \n",
    "---\n",
    "Implement a function that gets two images after axis alignment (using OpenCV's `warpPerspective`)\n",
    "and returns a union of the two. The union should be a simple overlay of one image on the other. Leave empty pixels painted black.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "8nzM_eG2jd-u"
   },
   "outputs": [],
   "source": [
    "def imageStitching(img1, warp_img2):\n",
    "    \"\"\"\n",
    "    Your code here\n",
    "    \"\"\"\n",
    "    return panoImg\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "qF2eYRCLjdHD"
   },
   "source": [
    "Inputs: `im1`,`warp_img2` are two colored images. \n",
    "\n",
    "Output: `panoImg` is the gathered output  panorama.\n",
    "\n",
    "* Use all the above functions to create a panorama image. Demonstrate and explain you results on the `./data/incline` images."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "2f1W9mxzkqo2"
   },
   "source": [
    "##### 3.5 - Several Images stitching:\n",
    "---\n",
    "* Show the results of the panoramas on the attached images of the beach (`./data/beach`) and Pena National Sintra Palace (`./data/sintra`) for the entire set of images.\n",
    "\n",
    "* Note: When using SIFT without RANSAC (next section), take the top K matches for estimating the homography. \n",
    "  * What happens if you don't do so? Why is that?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "b2BeuGdHkvSg"
   },
   "source": [
    "##### 3.6 - RANSAC:\n",
    "---\n",
    "Added bellow is an implementation of the RANSAC (**Ran**dom **Sa**mple **C**onsensus) algorithm. \n",
    "* Explain when it is needed and why.\n",
    "* Copmare between using RANSAC vs. not using it for creating the panoran images of the beach and SINTRA. Explain.\n",
    "* What could have been done to get better results?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "gtyM1Aa3kwtg"
   },
   "outputs": [],
   "source": [
    "def ransacH(p1, p2, nIter=..., tol=...):\n",
    "    N = p1.shape[1]\n",
    "    stacked_p2 = np.vstack((p2, np.ones(N)))\n",
    "\n",
    "    best_inliers_n = 0\n",
    "    best_inliers = []\n",
    "\n",
    "    for iter in range(nIter):\n",
    "        rand_idxs = np.random.choice(np.arange(N), 4, replace=False)\n",
    "        chosen_p1 = p1[:, rand_idxs]\n",
    "        chosen_p2 = p2[:, rand_idxs]\n",
    "        H2to1 = computeH(chosen_p1, chosen_p2)\n",
    "        p2in1 = H2to1 @ stacked_p2\n",
    "        p2in1 = p2in1 / p2in1[2, :]\n",
    "        p2in1 = p2in1[0:2, :]\n",
    "        L2dists = np.sqrt(np.sum((p2in1 - p1) ** 2, 0))\n",
    "        inliers = (p1[:, L2dists < tol], p2[:, L2dists < tol])\n",
    "        n_inliers = np.sum(L2dists < tol)\n",
    "        if n_inliers > best_inliers_n:\n",
    "            best_inliers_n = n_inliers\n",
    "            best_inliers = inliers\n",
    "\n",
    "    bestH = computeH(best_inliers[0], best_inliers[1])\n",
    "    return bestH"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "7J8Jmop3kyNO"
   },
   "source": [
    "Inputs: \n",
    "* `p1` and `p2` are matrices specifying point locations in each of the images and `p1[j]`,`p2[j]` are matched points between two images.\n",
    "\n",
    "* `nIter` is the number of iterations to run RANSAC  \n",
    "\n",
    "* `tol` is the tolerance value for considering a point to be an inlier. \n",
    "\n",
    " * Define your function so that these  `nIter` and  `tol` have reasonable default values.\n",
    "\n",
    "Outputs: \n",
    "* `bestH` is the homography model with the most inliers found during RANSAC"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "arFT1LNpkz6Y"
   },
   "source": [
    "##### 3.7 - Be Creative:\n",
    "---\n",
    "* Go out and take at least 3 pictures of a far distance object (e.g. a building), and use what you have learned to create a new excellent Panorama image.\n",
    "  * Add the resulted image to your report and to the `output` folder."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "fjrtT25dGnT-"
   },
   "source": [
    "---\n",
    "## <img src=\"https://img.icons8.com/dusk/64/000000/prize.png\" style=\"height:50px;display:inline\"> Credits\n",
    "* Images from <a href=\"http://www.image-net.org/\">Imagenet</a>\n",
    "* Videos from <a href=\"https://pixabay.com/videos/search/green%20screen/\">Pixabay</a>\n",
    "    * Dinosaur video from <a href=\"https://sites.google.com/a/sau17.net/modern-media/home/green-screen-animations\">Modern Media</a>\n",
    "* Icons from <a href=\"https://icons8.com/\">Icon8.com</a> - https://icons8.com"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "ee046746_hw2_segmentation_solution.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3.8.13 ('deep-learn')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  },
  "vscode": {
   "interpreter": {
    "hash": "9ba8c014002200c4cb7b1b24748f06bc0ff4fdc0d886ccf4fb4fcdd93e3da31d"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
